{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage Automatique : BE 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premier apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # pour que l'exécution soit déterministe\n",
    "\n",
    "# N est le nombre de données d'entrée\n",
    "# D_in est la dimension des données d'entrée\n",
    "# D_h le nombre de neurones de la couche cachée\n",
    "# D_out est la dimension de sortie (nombre de neurones de la couche de sortie\n",
    "\n",
    "N, D_in, D_h, D_out = 30, 2, 10, 3\n",
    "lr = 0.01 # learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(X, W, b):\n",
    "    I = X.dot(W)+b\n",
    "    O = 1/(1+np.exp(-I))\n",
    "    return (O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPass(X, Y, e, W, b, lr):\n",
    "    new_W = W + (lr*Y*(1-Y)*e) * X.T\n",
    "    new_b = b + (lr*Y*(1-Y)*e)\n",
    "    new_e = e*W\n",
    "    return new_W, new_b, new_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLoss(ii,lLoss):\n",
    "    if not ii%25000:\n",
    "        lLoss.append(np.square(Y_pred -Y).sum() / 2)\n",
    "    return lLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidDiff(Y):\n",
    "    return Y*(1-Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProgLastLayer(Y,Y_pred):\n",
    "    error=Y-Y_pred\n",
    "    delta=error*sigmoidDiff(Y_pred)\n",
    "    return error,delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProgHiddenLayer(sup_delta,sup_W,O):\n",
    "    error=sup_delta.dot(sup_W.T)\n",
    "    delta=error*sigmoidDiff(O1)\n",
    "    return error,delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFinalLayer(delta,lr,I,W,b,O):\n",
    "    W += lr * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declareLayer(w,h):\n",
    "    W = 2 * np.random.random((w,h)) - 1\n",
    "    b = np.zeros((1, h))\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.188484755517783, 3.5041585331587415, 3.2672074437128713, 2.781707810966515]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "lLoss=[]\n",
    "\n",
    "X = np.random.random((N, D_in))\n",
    "Y = np.random.random((N, D_out))\n",
    "\n",
    "\n",
    "'''Initialisation aléatoire des poids du réseau'''\n",
    "W1,b1=declareLayer(D_in,D_h)\n",
    "W2,b2=declareLayer(D_h,D_out)\n",
    "\n",
    "''' Apprentissage'''\n",
    "\n",
    "for ii in range (100000):\n",
    "    '''Forward '''\n",
    "\n",
    "    O1 = forwardPass(X,W1,b1)\n",
    "    O2 = forwardPass(O1,W2,b2)\n",
    "    Y_pred = O2\n",
    "\n",
    "    lLoss=printLoss(ii,lLoss)\n",
    "\n",
    "    ''' Backprog ''' \n",
    "    l2_error,l2_delta=backProgLastLayer(Y,Y_pred)\n",
    "    l1_error,l1_delta=backProgHiddenLayer(l2_delta,W2,O1)\n",
    "\n",
    "    W2 += lr * O1.T.dot(l2_delta)\n",
    "    b2 += lr * l2_delta.sum(axis=0)\n",
    "    O1 += lr * l2_delta.dot(W2.T)\n",
    "\n",
    "    b1 += lr * l1_delta.sum(axis=0)\n",
    "    W1 += lr * X.T.dot(l1_delta)\n",
    "print(lLoss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
