{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage Automatique : BE 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premier apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# from mnist import MNIST\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # pour que l'exécution soit déterministe\n",
    "\n",
    "# N est le nombre de données d'entrée\n",
    "# D_in est la dimension des données d'entrée\n",
    "# D_h le nombre de neurones de la couche cachée\n",
    "# D_out est la dimension de sortie (nombre de neurones de la couche de sortie\n",
    "\n",
    "N, D_in, D_h, D_out = 30, 2, 10, 3\n",
    "lr = 0.01 # learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(X, W, b):\n",
    "    I = X.dot(W)+b\n",
    "    O = 1/(1+np.exp(-I))\n",
    "    return (O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPass(X, Y, e, W, b, lr):\n",
    "    new_W = W + (lr*Y*(1-Y)*e) * X.T\n",
    "    new_b = b + (lr*Y*(1-Y)*e)\n",
    "    new_e = e*W\n",
    "    return new_W, new_b, new_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLoss(ii,lLoss):\n",
    "    if not ii%25000:\n",
    "        lLoss.append(np.square(Y_pred -Y).sum() / 2)\n",
    "    return lLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidDiff(Y):\n",
    "    return Y*(1-Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProgLastLayer(Y,Y_pred):\n",
    "    error=Y-Y_pred\n",
    "    delta=error*sigmoidDiff(Y_pred)\n",
    "    return error,delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProgHiddenLayer(sup_delta,sup_W,O):\n",
    "    error=sup_delta.dot(sup_W.T)\n",
    "    delta=error*sigmoidDiff(O1)\n",
    "    return error,delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declareLayer(w,h):\n",
    "    W = 2 * np.random.random((w,h)) - 1\n",
    "    b = np.zeros((1, h))\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateLayer(delta,I,W,b,lr,bFirstLayer):\n",
    "    new_W = W + lr * I.T.dot(delta)\n",
    "    b+= lr * delta.sum(axis=0)\n",
    "    if not bFirstLayer:\n",
    "        I = lr * delta.dot(W.T)\n",
    "    return new_W,b,I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning...:  71%|███████   | 70709/100000 [00:05<00:02, 13456.20it/s]"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "lLoss=[]\n",
    "\n",
    "X = np.random.random((N, D_in))\n",
    "Y = np.random.random((N, D_out))\n",
    "\n",
    "\n",
    "'''Initialisation aléatoire des poids du réseau'''\n",
    "W1,b1=declareLayer(D_in,D_h)\n",
    "W2,b2=declareLayer(D_h,D_out)\n",
    "\n",
    "''' Apprentissage'''\n",
    "\n",
    "for ii in tqdm(range(100000),desc='Learning...'):\n",
    "    '''Forward '''\n",
    "\n",
    "    O1 = forwardPass(X,W1,b1)\n",
    "    O2 = forwardPass(O1,W2,b2)\n",
    "    Y_pred = O2\n",
    "\n",
    "    lLoss=printLoss(ii,lLoss)\n",
    "\n",
    "    ''' Backprog ''' \n",
    "    l2_error,l2_delta=backProgLastLayer(Y,Y_pred)\n",
    "    l1_error,l1_delta=backProgHiddenLayer(l2_delta,W2,O1)\n",
    "\n",
    "    W2,b2,O1=updateLayer(l2_delta,O1,W2,b2,lr,bFirstLayer=False)\n",
    "    W1,b1,X=updateLayer(l1_delta,X,W1,b1,lr,bFirstLayer=True)\n",
    "    b1 += lr * l1_delta.sum(axis=0)\n",
    "    W1 += lr * X.T.dot(l1_delta)\n",
    "    \n",
    "print('Done !')\n",
    "\n",
    "print(\"Results : \")\n",
    "print(lLoss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
